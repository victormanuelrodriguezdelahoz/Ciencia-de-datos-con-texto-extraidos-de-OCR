{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3cd558",
   "metadata": {},
   "source": [
    "# PRUEBA TÉCNICA DE APRENDIZAJE AUTOMÁTICO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6436d87",
   "metadata": {},
   "source": [
    "Científico de datos: Victor Manuel Rodriguez De La Hoz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf6f7a",
   "metadata": {},
   "source": [
    "# Librerías y lectura de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28718d94",
   "metadata": {},
   "source": [
    "Importe de las librerías iniciales para la realización del trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5cbf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0607a",
   "metadata": {},
   "source": [
    "Se hace la lectura de los archivos que se alojan en cada una de las carpetas entregadas para la prueba. \n",
    "Todos los archivos se agregan en orden a un dataframe que contiene dos columnas , una columna para el texto de cada muestra que se llama \"Mensajes\" y otra columna que indica cada texto de que carpeta es dándole un valor ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f88928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicma\\AppData\\Local\\Temp\\ipykernel_7184\\3921569738.py:86: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append([{\"Mensajes\": v} for v in listafactura], ignore_index=True)\n",
      "C:\\Users\\vicma\\AppData\\Local\\Temp\\ipykernel_7184\\3921569738.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append([{\"Mensajes\": e} for e in listahistoria], ignore_index=True)\n",
      "C:\\Users\\vicma\\AppData\\Local\\Temp\\ipykernel_7184\\3921569738.py:88: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df=df.append([{\"Mensajes\": h} for h in listaprocedimientos], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "listaanexo=[]\n",
    "listafactura=[]\n",
    "listaanexoconteo1=[]\n",
    "listafacturaconteo1=[]\n",
    "listahistoria=[]\n",
    "listahistoriaconteo1=[]\n",
    "listaprocedimientos=[]\n",
    "listaprocedimientosconteo1=[]\n",
    "initial_count1 = 0\n",
    "initial_count2 = 0\n",
    "initial_count3 = 0\n",
    "initial_count4 = 0\n",
    "dir1 = \"Anexos\"\n",
    "dir2 = \"Factura\"\n",
    "dir3= \"Historia\"\n",
    "dir4= \"Procedimientos\"\n",
    "for path in os.listdir(dir1):\n",
    "    if os.path.isfile(os.path.join(dir1, path)):\n",
    "        initial_count1 += 1\n",
    "for path in os.listdir(dir2):\n",
    "    if os.path.isfile(os.path.join(dir2, path)):\n",
    "        initial_count2 += 1\n",
    "for path in os.listdir(dir3):\n",
    "    if os.path.isfile(os.path.join(dir3, path)):\n",
    "        initial_count3 += 1\n",
    "for path in os.listdir(dir4):\n",
    "    if os.path.isfile(os.path.join(dir4, path)):\n",
    "        initial_count4 += 1\n",
    "for i in range(0,(initial_count1-1)):\n",
    "    if i==0:\n",
    "        b=str(0)+str(0)+str(0)\n",
    "    elif 0<i<10:\n",
    "        b=str(0)+str(0)+str(i)\n",
    "    elif 10<=i<100:\n",
    "        b=str(0)+str(i)\n",
    "    else:\n",
    "        b=str(i)\n",
    "    name='Anexos/doc_0'+b+'.txt'\n",
    "    with codecs.open(name, encoding='latin-1') as f:\n",
    "        mensaje = str(f.read())\n",
    "        listaanexo.append(mensaje)\n",
    "        listaanexoconteo1.append('anexo')\n",
    "for i in range(0,(initial_count2-1)):\n",
    "    if i==0:\n",
    "        b=str(0)+str(0)+str(0)\n",
    "    elif 0<i<10:\n",
    "        b=str(0)+str(0)+str(i)\n",
    "    elif 10<=i<100:\n",
    "        b=str(0)+str(i)\n",
    "    else:\n",
    "        b=str(i)\n",
    "    name1='Factura/doc_0'+b+'.txt'\n",
    "    with codecs.open(name1, encoding='latin-1') as f:\n",
    "        mensaje1 = str(f.read())\n",
    "        listafactura.append(mensaje1)\n",
    "        listafacturaconteo1.append('factura')\n",
    "for i in range(0,(initial_count3-1)):\n",
    "    if i==0:\n",
    "        b=str(0)+str(0)+str(0)\n",
    "    elif 0<i<10:\n",
    "        b=str(0)+str(0)+str(i)\n",
    "    elif 10<=i<100:\n",
    "        b=str(0)+str(i)\n",
    "    else:\n",
    "        b=str(i)\n",
    "    name2='Historia/doc_0'+b+'.txt'\n",
    "    with codecs.open(name2, encoding='latin-1') as f:\n",
    "        mensaje2 = str(f.read())\n",
    "        listahistoria.append(mensaje2)\n",
    "        listahistoriaconteo1.append('historia')\n",
    "for i in range(0,(initial_count4-1)):\n",
    "    if i==0:\n",
    "        b=str(0)+str(0)+str(0)\n",
    "    elif 0<i<10:\n",
    "        b=str(0)+str(0)+str(i)\n",
    "    elif 10<=i<100:\n",
    "        b=str(0)+str(i)\n",
    "    else:\n",
    "        b=str(i)\n",
    "    name3='Procedimientos/doc_0'+b+'.txt'\n",
    "    with codecs.open(name3, encoding='latin-1') as f:\n",
    "        mensaje3 = str(f.read())\n",
    "        listaprocedimientos.append(mensaje3)\n",
    "        listaprocedimientosconteo1.append('procedimientos')\n",
    "df.insert(0, \"Mensajes\", listaanexo, allow_duplicates=False)\n",
    "df=df.append([{\"Mensajes\": v} for v in listafactura], ignore_index=True)\n",
    "df=df.append([{\"Mensajes\": e} for e in listahistoria], ignore_index=True)\n",
    "df=df.append([{\"Mensajes\": h} for h in listaprocedimientos], ignore_index=True)\n",
    "listaanexoconteo1+=listafacturaconteo1\n",
    "listaanexoconteo1+=listahistoriaconteo1\n",
    "listaanexoconteo1+=listaprocedimientosconteo1\n",
    "df.insert(1, \"Valor\", listaanexoconteo1, allow_duplicates=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2773bde",
   "metadata": {},
   "source": [
    "Como se puede ver en la visualización del dataframe se aprecia que los mensajes o textos de cada muestra tienen un texto desordenado un poco difícil de entender , para esto más adelante se realizará una limpieza del texto utilizando técnicas de NLP y en la lectura de los archivos se estudió el tipo de codificación con el que se apropia más el texto para que fuera más entendible cada texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb49eb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mensajes</th>\n",
       "      <th>Valor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hee 9999-99 Ga Fo Nt D9\\r\\n\\r\\nClinica v.99\\r\\...</td>\n",
       "      <td>anexo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Historia: 999999 - Ingreso: 99 Pag 999 de 999\\...</td>\n",
       "      <td>anexo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/99/99 99:99 MATRIX - [REPORTE PARA VER EL DE...</td>\n",
       "      <td>anexo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clinica\\r\\n\\r\\n \\r\\n\\r\\nauana COMPROBANTE DE P...</td>\n",
       "      <td>anexo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Historia: 999999 - Ingreso: 99 Pag 9 de 99\\r\\n...</td>\n",
       "      <td>anexo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>GUSTAVO ALBERTO ARCILA GOMEZ\\r\\n\\r\\nHistoria: ...</td>\n",
       "      <td>procedimientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>Historia: 99999 - Ingreso: 99 Pag 99 de 999\\r\\...</td>\n",
       "      <td>procedimientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>Historia: 999999 - Ingreso: 99\\r\\n\\r\\nrire!\\r\\...</td>\n",
       "      <td>procedimientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>Historia: 99999 - Ingreso: 9 Pag 99 de 99\\r\\n\\...</td>\n",
       "      <td>procedimientos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>Historia: 999999 - Ingreso: 99 Pag 9de9\\r\\n\\r\\...</td>\n",
       "      <td>procedimientos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1678 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Mensajes           Valor\n",
       "0     Hee 9999-99 Ga Fo Nt D9\\r\\n\\r\\nClinica v.99\\r\\...           anexo\n",
       "1     Historia: 999999 - Ingreso: 99 Pag 999 de 999\\...           anexo\n",
       "2     9/99/99 99:99 MATRIX - [REPORTE PARA VER EL DE...           anexo\n",
       "3     Clinica\\r\\n\\r\\n \\r\\n\\r\\nauana COMPROBANTE DE P...           anexo\n",
       "4     Historia: 999999 - Ingreso: 99 Pag 9 de 99\\r\\n...           anexo\n",
       "...                                                 ...             ...\n",
       "1673  GUSTAVO ALBERTO ARCILA GOMEZ\\r\\n\\r\\nHistoria: ...  procedimientos\n",
       "1674  Historia: 99999 - Ingreso: 99 Pag 99 de 999\\r\\...  procedimientos\n",
       "1675  Historia: 999999 - Ingreso: 99\\r\\n\\r\\nrire!\\r\\...  procedimientos\n",
       "1676  Historia: 99999 - Ingreso: 9 Pag 99 de 99\\r\\n\\...  procedimientos\n",
       "1677  Historia: 999999 - Ingreso: 99 Pag 9de9\\r\\n\\r\\...  procedimientos\n",
       "\n",
       "[1678 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c585e06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[499, 499, 439, 241]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Valor'].value_counts().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab005751",
   "metadata": {},
   "source": [
    "# Separación de datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58f01a",
   "metadata": {},
   "source": [
    "Para dividir las muestras en un conjunto de entrenamiento y uno de testeo se utiliza la librería sklearn la cual tiene un módulo que nos ayuda a dividir los datos los dos conjuntos necesarios.\n",
    "En este caso se utiliza la columna \"Mensajes\" como los datos de texto y la columna \"Valor\" como el label que nos indica el valor de cada mensaje.\n",
    "Como se puede ver , de las 1678 filas del dataframe se toman 1258 muestras como entrenamiento y 420 como datos de testeo, utilizando esta herramienta nos aseguramos de que se hayan distribuido proporcionalmente los datos dependiendo de cuantas muestras por categoría hay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7038e0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 1258, testing examples 420\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(df[\"Mensajes\"], df[\"Valor\"],stratify=df[\"Valor\"])\n",
    "print(f\"Training examples: {len(train_text)}, testing examples {len(test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e752f0f",
   "metadata": {},
   "source": [
    "# Preprocesamiento de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f64f0c",
   "metadata": {},
   "source": [
    "Inicialmente se crea esta función ,la cual inicialmente va a recibir como dato de entrada una oración o en este caso el texto de cada muestra , utilizando este texto se utiliza el método split y realizando un condicional se revisan los caracteres que tiene cada conjunto de caracteres en el texto y procede a guardar en un nuevo arreglo solo los caracteres que no fueran signos de puntuación , estos datos se guardan pasados a minúsculas en este arreglo para posteriormente pasar cada conjunto de caracteres juntos sin espacio a un nuevo arreglo que guardará cada conjunto de caracteres que estén pegados como un dato nuevo.\n",
    "\n",
    "En pocas palabras esta función separa por lo que puede ser una palabra el texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6666afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "def tokenize(sentence):\n",
    "    tokens = []\n",
    "    for token in sentence.split():\n",
    "        new_token = []\n",
    "        for character in token:\n",
    "            if character not in punctuation:\n",
    "                new_token.append(character.lower())\n",
    "        if new_token:\n",
    "            tokens.append(\"\".join(new_token))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb28a9",
   "metadata": {},
   "source": [
    "En esta parte se puede apreciar lo que hace la función con los textos del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bd678ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [hee, 999999, ga, fo, nt, d9, clinica, v99, le...\n",
       "1    [historia, 999999, ingreso, 99, pag, 999, de, ...\n",
       "2    [99999, 9999, matrix, reporte, para, ver, el, ...\n",
       "3    [clinica, auana, comprobante, de, prestacion, ...\n",
       "4    [historia, 999999, ingreso, 99, pag, 9, de, 99...\n",
       "Name: Mensajes, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()[\"Mensajes\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebedb45",
   "metadata": {},
   "source": [
    "Ahora se utiliza la función anteriormente creada junto al módulo \"CountVectorizer”, el cual nos va a ayudar a crear una instancia que utilice la función anteriormente creada para limpiar texto y separar palabras, y a la vez este nuevo módulo revisa que la palabra aparezca una vez y le da un valor binario dentro de una lista de vocabulario conocido.\n",
    "\n",
    "Después los datos de entrenamiento y de testeo se transforman utilizando esta función anteriormente creada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0011acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "real_vectorizer1 = CountVectorizer(tokenizer = tokenize, binary=True)\n",
    "train_X = real_vectorizer1.fit_transform(train_text)\n",
    "test_X = real_vectorizer1.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f6984",
   "metadata": {},
   "source": [
    "# Algoritmo de aprendizaje automático:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067fb557",
   "metadata": {},
   "source": [
    "# Máquinas de vector de soporte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6bd7f",
   "metadata": {},
   "source": [
    "Para el algoritmo de aprendizaje automático se utilizó la librería sklearn con el algoritmo SVM(Maquinas de vector de soporte) el cual se ejecuta con el módulo \"Linear SVC\" el cual es el algoritmo ejecutado de forma linear. Este está basado en las máquinas de soporte clasificatorio.\n",
    "Este algoritmo funciona haciendo un hiperplano que separa las observaciones o características propias de las categorías que se tratan y se dejan un umbral \"C\" el cual va a ser la distancia que puede estar separada una muestra del hiperplano de una categoría; por tanto mientras más grande sea el valor de C ,menos muestras van a pasar y las muestras se tendrán que acercar mucho más a la linealidad y a la perfección , mientras que a menor C más muestras van a pasar y el castigo en los pesos va a ser menor . \n",
    "\n",
    "Este algoritmo no se usa mucho en otras aplicaciones debido a que el sistema tendrá que tender a ser lineal y conlleva un coste computacional grande cuando hay muchos predictores.\n",
    "\n",
    "La razón por la que se escoge este modelo es porque al ser un modelo en que cada categoría se diferencia por la ocurrencia de ciertas palabras en específico, el análisis de estas variables que podrían considerarse categórica puede acercarse de una forma más cercana a un sistema lineal y el modelo del hiperplano se acerca más a lo requerido. Por otro lado, este algoritmo tiene un costo computacional que depende mucho más de las observaciones que de los predictores y al tener pocas muestras y 4 categorías, el número de observaciones fue corto. Otro dato a tener en cuenta es que es un algoritmo que puede reducir el \"overfitting\" si se tiene en cuenta en su modo estándar, ósea que su margen no sea lleva a un máximo de hiperplano, esto quiere decir que el control de que las categorías sean memorizadas por el modelo es poco probable porque depende del umbral que tiene el modelo. A partir de esta decisión me documenté y vi que es una técnica usada en NLP.\n",
    "\n",
    "También se pudo haber utilizado \"random forest\", \"árbol de decisión simple”. No se utilizan regresiones por el tipo de salida esperada aunque en una posible mejora del algoritmo se puede predecir implementar con más características, columnas o información .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88077afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=1.5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=1.5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(C=1.5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(train_X, train_labels)\n",
    "LinearSVC(C=1.5, class_weight=None, dual=True, fit_intercept=True,\n",
    "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "          verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e80c2",
   "metadata": {},
   "source": [
    "# Métricas y explicación:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8c04d",
   "metadata": {},
   "source": [
    "Después de tener el modelo listo, primero que todo se utilizan las herramientas de la librería para medir las métricas usando los datos de prueba, posteriormente se probara el modelo manualmente.\n",
    "\n",
    "Para la exactitud del modelo se obtuvo un 95% de precisión con un \"C=1\" entonces se incrementó y se varió la penalidad hasta llegar a una exactitud de 97.1429% con un \"C=1.5\" .\n",
    "\n",
    "La exactitud en el modelo se entiende como que tan cerca los valores obtenidos estuvieron del valor real de la categoría, al estar en un 97% se entiende que la gran mayoría estuvieron cercanos , no hubo errores notables de overfitting y  se podría mejorar varían ciertos parámetros del modelo, la cantidad de muestras y la codificación de los datos para mejorar el NLP hecho.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9604a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.1429%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicciones = classifier.predict(test_X)\n",
    "accuracy = accuracy_score(test_labels, predicciones)\n",
    "print(f\"Accuracy: {accuracy:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64811f2d",
   "metadata": {},
   "source": [
    "Esta herramienta de la librería es un reporte de las métricas separadas entre cada categoría. Como se puede observar, aunque se tenían menos muestras en los anexos el valor de su precisión es el segundo más alto. La precisión se entiende como la menor dispersión entre valores obtenidos, por tanto se podría decir que los anexos tienen características bastante diferenciadores de las otras tres categorías y que posiblemente el a pocas muestras puede memorizar un poco, pero después de ver el valor de \"recall\" se puede decir que no memorizo porque el \"recall\" es la cantidad de aciertos sobre el número de muestras evaluadas, por tanto los anexos y las facturas acertaron mucho más que los demás. F1-score es la relación es una formula en la que se utiliza la precisión y el \"recall\", en este caso los valores fueron buenos, exceptuando el valor de los de historia que fueron los más bajos en recall y f1-score , lo cual puede decir que puede ser grande la posibilidad de desacierto o falso positivo o falso negativo en el conjunto de historia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7176f131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anexo       0.98      1.00      0.99        60\n",
      "       factura       1.00      1.00      1.00       125\n",
      "      historia       0.97      0.92      0.94       110\n",
      "procedimientos       0.94      0.98      0.96       125\n",
      "\n",
      "      accuracy                           0.97       420\n",
      "     macro avg       0.97      0.97      0.97       420\n",
      "  weighted avg       0.97      0.97      0.97       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "reporte_metricas = classification_report(test_labels, predicciones)\n",
    "print(reporte_metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9e5e8",
   "metadata": {},
   "source": [
    "La matriz nos dice por columna la cantidad de predicciones y por fila en que instancia fue real, hay que relacionar las predicciones por categoría y si fueron reales o falsas.\n",
    "En esta matriz de confusión se pueden visualizar la cantidad de aciertos y desaciertos que tuvo el modelo. Se tuvo un desacierto en la primera categoría la cual fue confundida con la tercera categoría, la tercera categoría tuvo 3 desaciertos en la que se le dieron estos resultados a la cuarta categoría y la cuarta categoría tuvo 8 desaciertos donde estos se le dieron a la tercera categoría. Todos estos datos se corroboran más adelante en la métrica de \"zero_one_loss\".\n",
    "\n",
    "De este análisis se puede deducir dos cosas directamente, la categoría tres guarda cierta relación con la categoría 1 y la categoría 4. Y la categoría 4 fue la más susceptible a errores por tanto es la que tendría que ser alimentada con más muestras o mirar la organización de los datos, o la forma de modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c00b196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 60   0   0   0]\n",
      " [  0 125   0   0]\n",
      " [  1   0 101   8]\n",
      " [  0   0   3 122]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matriz_confusion = confusion_matrix(test_labels, predicciones)\n",
    "print(matriz_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a6180",
   "metadata": {},
   "source": [
    "Otra métrica importante en ingles se le conoce como \"zero_one_loss\", este en pocas palabras es el conteo de la cantidad de muestras en los datos de prueba que no acertaron de categoría. Si la muestra es correcta obtiene un cero, si es incorrecta obtiene un 1, estos números por muestra son sumados y este da el valor de la métrica. En este caso se obtiene zero_one_loss=12. Lo cual es un valor alto dentro del conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18329956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "zero_one_loss = zero_one_loss(test_labels, predicciones,normalize=False)\n",
    "zero_one_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a29ce6",
   "metadata": {},
   "source": [
    "# Respuestas a preguntas de la prueba:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a663e",
   "metadata": {},
   "source": [
    " ¿Qué efectos negativos puede tener el desbalance de muestras en el \n",
    "rendimiento de un clasificador, evidenció algún efecto negativo en este \n",
    "caso?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d8cf2",
   "metadata": {},
   "source": [
    "Los efectos negativos que puede tener el desbalance de muestras son primero que todo que es probable que el clasificador no tengo un nivel igual de alto de exactitud que con categorías con mucha mayor cantidad de muestras(mayoritario). Lo segundo que puede pasar es que el modelo no encuentre patrones o algún dato de análisis de la categoría y termine dando por acertado solo aquellos datos exclusivamente similares a los datos de entrenamiento. Por lo general en las métricas de evaluación se suele ver en estos casos que la categoría desbalanceada tiene un recall bajo y a la vez un f1-score bajo, tal vez puede tener una precisión medianamente alta. En este caso la categoría uno la cual fue la que tenía pocas muestras en comparación a las demás categorías no mostro un comportamiento marcado de que tuviera un desbalance porque sus métricas fueron buenas ; esto pudo deberse tal vez a que esta categoría tenía palabras bastante significativas y diferenciadoras de las otras categorías , la organización de sus datos pudo haber sido mejor y por ende el modelo entendió mejor la categoría tomando bien los pesos y perdidas con las que se probó el modelo dejándola así como una de las dos mejores categorías en el modelo con mejores métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9c649",
   "metadata": {},
   "source": [
    "¿Cómo se podrían mitigar los efectos negativos del desbalance de \n",
    "muestras?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b94b3e",
   "metadata": {},
   "source": [
    "Los efectos negativos del desbalance de muestras se pueden mitigar de varias maneras.\n",
    "Primero: En los parámetros del modelo se puede diferenciar los pesos de castigo de las categorías mayoritarias y las categorías minoritarias, o también dependiendo del modelo se pueden variar las pérdidas para mejorar este desbalance.\n",
    "\n",
    "Segundo: Utilizar varios modelos en las que sus salidas sean analizadas y ajustadas para tener mayor porcentaje de precisión.\n",
    "\n",
    "Tercero: Se pueden crear muestras sintéticas utilizando algoritmos de interpolación o relacionados que creen datos basados en un grupo minoritario de muestras. Lo no ideal es que esto puede alterar el orden natural de los datos por tal motivo es importante siempre conocer lo más posible el contexto de la categoría y el proyecto en general.\n",
    "\n",
    "Cuarto: Modificando el dataset, esto quiere decir que se pueden reducir la cantidad de muestras del grupo mayoritario basado en el número de muestras del grupo minoritario. Esto también debe hacerse inteligentemente y tratando de que no sea totalmente manual ni totalmente aleatorio porque se pueden perder muestras significativas de la categoría mayoritaria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5004bd58",
   "metadata": {},
   "source": [
    "# Pruebas manuales:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da91190",
   "metadata": {},
   "source": [
    "A continuación, se muestran pruebas manuales que se hicieron del modelo utilizando archivos aleatorios de las carpetas suministradas.\n",
    "Como se puede ver las pruebas que se hicieron manualmente fueron acertadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97bbf8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = [ \"\"\"LUIS ALBERTO JARAMILLO FRANCO Pag 9 de 99\n",
    "Historia: 999999 - Ingreso: 9 9\n",
    "\n",
    "ZEl procedimiento realizado se deriva de un caso que indique presencia de politrauma? : |\n",
    "No\n",
    "\n",
    "Profilaxis Prequirargica\n",
    "\n",
    "éRealiz9 prescripcion de profilaxis prequirurgica antibidtica? : Si |\n",
    "\n",
    "cefazolina (uso de protesisi)\n",
    "\n",
    "FIRMADO ELECTRONICAMENTE POR : CARLOS HERNANDO MORALES URIBE Identificacion : CC 99999999 Registro : 9999 Profesion\n",
    "o Especialidad : CIRUGIA GENERAL Fecha : 9999-99-99 Hora : 99:99:99\n",
    "\n",
    "I Wok,\n",
    "\n",
    "Generado: 9999-99-99 - 99:99:99 por: MARIA JULIANA SOTO ARIAS\n",
    "\f",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a43c561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_X = real_vectorizer1.transform(prueba)\n",
    "predicciones1 = classifier.predict(prueba_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e2a17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['procedimientos'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicciones1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7ab7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba2=[\"\"\"Historia: 999999 - Ingreso: 9\n",
    "\n",
    "Pag 99 de 999\n",
    "\n",
    "Paciente de 99 afios , sin evaluaciones previas por psiquiatria , en el momento en periodo de duelo de fallecimiento de su esposo hace 9\n",
    "\n",
    "meses.\n",
    "No se evidencian sintomas ansiosos , afectivos , sin cambios de patrones biologicos .\n",
    "Se brinda acompafiamiento\n",
    "\n",
    "Por el momento no requiere psicofarmacos ni manejo intrahospitalario por psiquiatria\n",
    "Se cierra [C y se solicita vatoracton por psicologia para acompaftamiento\n",
    "\n",
    "Se explica a paciente y acompaftante\n",
    "\n",
    "Conducta y Plan\n",
    "Por el momento no requiere psicofarmacos ni manejo intrahospitalario por psiquiatria\n",
    "Se cierra IC y se solicita vatoracion por psicologia para acompafamiento\n",
    "\n",
    "Se explica a paciente y acompaftante\n",
    "\n",
    "Diagnosticos CIE-99\n",
    "\n",
    "(9) M999 OSTEOMIELITIS DE VERTEBRA.\n",
    "\n",
    "informacion Suministrada al Paciente y/o Familia\n",
    "\n",
    "Por el momento no requiere psicofarmacos ni manejo intrahospitalario por psiquiatria\n",
    "Se cierra [C y se solicita vatoracion por psicologia para acompaftamiento\n",
    "Se explica a paciente y acompafante\n",
    "\n",
    "CLASIFICACION TIPO DE PACIENTE\n",
    "\n",
    "CLASIFICACION TIPO DE PACIENTE :\n",
    "NO COVID\n",
    "\n",
    "FIRMADO ELECTRONICAMENTE POR : PAOLA GUTIERREZ BRICENO Identiftcacton : CC 99999999 Registro : 9999999. Profesién o\n",
    "\n",
    "Especialidad : SIQUIATRIA Fecha : 9999-99-99 Hora : 99:99:99\n",
    "\n",
    "*** EVOLUCION ***\n",
    "\n",
    "Edad\n",
    "\n",
    "99 Afios 9 Meses 9 Dias\n",
    "\n",
    "Fecha y Hora\n",
    "\n",
    "9999-HOSP. PISO 9 - TORRE 9 Hab. 999\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "9999-99-99 99:99:99\n",
    "\n",
    "Evolucion Diaria\n",
    "\n",
    "Generado: 9999-99-99 - 99:99:99 por: GIOVANNA HERNANDEZ\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd0cd151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['historia'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba_X2 = real_vectorizer1.transform(prueba2)\n",
    "predicciones2 = classifier.predict(prueba_X2)\n",
    "predicciones2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afb49f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba3=[\"\"\" \n",
    "\n",
    "ADMINISTRADORA DE LOS RECURSOS DEL SISTEMA GENERAL\n",
    "DE SEGURIDAD SOCIAL EN SALUD - ADRES\n",
    "\n",
    "Informacion de Afiliados en la Base de Datos Unica de Afiliados al Sistema de Seguridad Social en Salud\n",
    "\n",
    "Resultados de la consulta\n",
    "\n",
    "Informacion Basica del Afiliado :\n",
    "\n",
    "COLUMNAS DATOS\n",
    "TIPO DE IDENTIFICACION cc\n",
    "NUMERO DE\n",
    "IDENTIFICACION 9999999999\n",
    "NOMBRES SILVIA EUGENIA\n",
    "APELLIDOS MOLANO MAYA\n",
    "FECHA DE NACIMIENTO apes ne\n",
    "DEPARTAMENTO ANTIOQUIA\n",
    "MUNICIPIO LA CEJA\n",
    "Datos de afiliacién :\n",
    "ESTADO ENTIOAD REGIMEN FECHA DE FECHA DE ~~ ~=TIPO DE AFILIADO\n",
    "AFILIACION FINALIZACION DE\n",
    "EFECTIVA AFILIACION\n",
    "COOMEVA\n",
    "ENTIDAD\n",
    "ACTIVO Ree DE CONTRIBUTIVO 99/99/9999 —«- 99/99/9999 Ss COTIZANTE\n",
    "\"COOMEVA\n",
    "EPS. S.A.\"\n",
    "eengge | omsanoes | estén se | sg 9a.99\n",
    "\n",
    "La informacion registrada en esla pagina es reflejo de lo reporlado por las Enlidades en cumplimiento de la Resolucion 9999 de 9999.\n",
    "\n",
    "Respecto a las fechas de afiliacian contenidas en esta consulta, se aclara que la Fecha de Afiliacion Efectiva hace referencia a la fecha\n",
    "en la cual inicia la afiliaci9n para el usuario, la cual fue reporiada por la EPS 9 EOC, sin importar que haya estado en el Régimen\n",
    "Contributivo o en el Regimen Subsidiado en dicha entidad. Ahora bien, la Fecha de Finalizacion de Afiliacian, establece el término de la\n",
    "afiliacién a la entidad de acuerdo con Ia fecha de la novedad que haya presentado la EPS o EOC. A su vez se aclara que la fecha de\n",
    "99/99/9999 determina que el afiliado se encuentra vinculado con la entidad que genera la consulta.\n",
    "\n",
    "La responsabilidad por la calidad de los datos y la informacion reportada a la Base de Dalos Unica de Afiliados - BDUA, junto con el reporle\n",
    "oportuno de las novedades para actualizar la BDUA, corresponde directamente a su fuente de informacion; en este caso de las EPS, EOC y\n",
    "EPS-S.\n",
    "\n",
    "Esta informacion se debe utilizar por parte de las entidades y los prestadores de servicios de salud, como complemento al marco\n",
    "legal y técnico definido y nunca como motivo para denegar la prestacion do los servicios de salud a los usuarios.\n",
    "\n",
    "Si usted encuentra una inconsistencia en la informacién publicada en ésta pagina, por favor remitase a la EPS en la cual se encuentre\n",
    "afiliado y solicite la correccién de la informacion inconsistente sobre su afiliacion, Una vez realizada esta actividad, la EPS debe remitir la\n",
    "novedad correspondiente a la ADRES, conforme lo establece la normatividad vigente.\n",
    "\n",
    "IMPRIMIR CERRAR VENTANA\n",
    "\n",
    "\n",
    " \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80bfac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anexo'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba_X3 = real_vectorizer1.transform(prueba3)\n",
    "predicciones3 = classifier.predict(prueba_X3)\n",
    "predicciones3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24383b",
   "metadata": {},
   "source": [
    "# Conclusiones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e37861",
   "metadata": {},
   "source": [
    "Se concluye que el modelo el cual tiene cuatro categorías funciona de buena manera en su presentación inicial y puede tener mejoras a futuro.\n",
    "\n",
    "El dato desbalanceado logro adaptarse al modelo de buena manera y se propone mejorar las muestras de la categoría 3 y 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ee895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
